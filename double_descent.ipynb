{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f67a5f7",
   "metadata": {},
   "source": [
    "# Double Descent Phenomenon\n",
    "\n",
    "Egy model kapacitásának növelése egy adott pontig csökkenti a teszt hibát, ezt követően a teszt hiba nő, majd az interpolációs küszöbön túl, a hiba ismét elkezd csökkenni.\n",
    "\n",
    "[Schaeffer, Rylan, et al. (2023)](https://arxiv.org/abs/2303.14151)\n",
    "\n",
    "[Belkin, Mikhail, et al. (2019)](https://arxiv.org/abs/1812.11118)\n",
    "\n",
    "![alt](double_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112231a",
   "metadata": {},
   "source": [
    "## Jelölés\n",
    "\n",
    "$ \\mathcal{D} = \\{(x_1, y_1),...,(x_n,y_n)\\},\\ |\\mathcal{D}| = N $\n",
    "\n",
    "$ (x_i,y_i) \\in \\mathbb{R}^d \\times \\mathbb{R} $\n",
    "\n",
    "$ f: \\mathbb{R}^d \\mapsto \\mathbb{R} $ \n",
    "\n",
    "$ f \\in \\mathcal{H} $ függvény család\n",
    "\n",
    "<!-- Van egy adathalmazunk ami alapján kiválasztunk egy h függvényt egy H függvénycsaládból amivel új adatok cimkéit tudjuk prediktálni -->\n",
    "\n",
    "## Classical Bias-Variance trade-off\n",
    "\n",
    " 1. Ha $\\mathcal{H}$ kapacitása kicsi (_high bias_), akkor minden $ f \\in \\mathcal{H} $ nem illeszkedik megfelelő mértékben a tanulási adatokra (_under-fitting_), ezért új adatokra is rossz eredményeket fog adni.\n",
    " 2. Ha $\\mathcal{H}$ kapacitása nagy (_high variance_), akkor léteznek $ f \\in \\mathcal{H} $ amelyek tökéletesen illeszkednek a tanulási adatokra (_over-fitting_), viszont nem képesek általánosítani, ezért új adatokra rossz eredméyeket fognak adni.\n",
    "\n",
    "Ezek alapján $\\mathcal{H}$-t úgy kell kiválasztani, hogy a kapacitása a _sweet spot_-ban legyen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81acde3",
   "metadata": {},
   "source": [
    "## \"Modern\" Interpolating Regime\n",
    "\n",
    "![alt](modern.png)\n",
    "\n",
    "Ha $ \\mathcal{H} $ kapacitását növeljük, az interpolációs küszöbön túl <!-- Az a pont ahol létezik legalább 1 függvény H-ban amire a train hiba 0 -->, a tanulási hiba 0 marad, viszont a test hiba ismét elkezd csökkenni. Ez ellentmond a bias-variance trade-off elvnek.\n",
    "\n",
    "__Egyszeűsített intuitív magyarázat.__ Az interpolációs küszöbön 1 függvény létezik, ami képes tökéletesen illeszkedni a tanulási adatokra, annak a valószínűsége, hogy ez a függvény új adatokra is illeszkedni fog eléggé alacsony. Az interpolációs küszöbön túl, több függvény fog tökéletesen illeszkedni az adatokra, tehát annak a valószínűsége, hogy létezik ezek között legalább egy olyan amely új adatokra is megfelő predikciókat ad, nagyobb.\n",
    "\n",
    "<!-- Ez nem jelenti azt, hogy ez a függvény lesz valójában kiválasztva -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9bd306",
   "metadata": {},
   "source": [
    "### Vizuális Intuició Polinomiális Regresszióval\n",
    "\n",
    "$ y: \\mathcal{R} \\mapsto \\mathcal{R} $\n",
    "\n",
    "$ y(x) = 2x + cos(25x) $\n",
    "\n",
    "$ \\phi_P: \\mathcal{R} \\mapsto \\mathcal{R}^P $\n",
    "\n",
    "$ \\phi_P(x) = \\left[ \\begin{array}{c} \\phi_1(x) \\\\ \\phi_2(x) \\\\\\vdots \\\\ \\phi_P(x) \\end{array} \\right] $\n",
    "\n",
    "$ y \\approx \\phi_P(x) \\cdot \\Theta_P $\n",
    "\n",
    "<!-- Baloldalt: P < N a függvény under-fittel -->\n",
    "<!-- Középen: P = N a függvény over-fittel -->\n",
    "<!-- Jobb oldal P > N double descent -->\n",
    "\n",
    "![alt](polinomial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1451d6a5",
   "metadata": {},
   "source": [
    "## Matematikai Intuició Lineáris Regresszióval\n",
    "\n",
    "$ Y = X \\cdot \\Theta $\n",
    "\n",
    "Lineáris regresszió esetében $ P = D $. Mivel a paraméterek száma nem növelhető, az adatok számát fogjuk csökkenteni.\n",
    "\n",
    " 1. Ha $ P < N $\n",
    "\n",
    "    $ \\hat{\\Theta}_{under} = arg \\underset{\\Theta}{min} || X \\Theta - Y ||^2 $\n",
    "\n",
    "    Megoldás: $ \\hat{\\Theta}_{under} = (X^TX)^{-1}X^TY $\n",
    "\n",
    " 2. Ha $ P > N $\n",
    "\n",
    "    $ \\hat{\\Theta}_{over} = arg \\underset{\\Theta}{min} || \\Theta ||^2,\\ \\forall n \\in \\{1,...,N\\}\\ x_n \\cdot \\Theta = y_n $\n",
    "\n",
    "    Megoldás: $ \\hat{\\Theta}_{over} = X^T(XX^T)^{-1}Y $\n",
    "\n",
    "$ \\hat{y}_{test,under} = x_{test} \\cdot \\hat{\\Theta}_{under} = x_{test} \\cdot (X^TX)^{-1}X^TY $\n",
    "\n",
    "$ \\hat{y}_{test,over} = x_{test} \\cdot \\hat{\\Theta}_{over} = x_{test} \\cdot X^T(XX^T)^{-1}Y $\n",
    "\n",
    "Jelöljuk $ \\Theta^* $-al az ismeretlen ideális paramétereket amikre minimális a teszt hiba\n",
    "\n",
    "Tehát $ Y = X\\Theta^* + E $, ahol $ E $ az adat megtanulhatatlan része\n",
    "\n",
    "Ezt használva írjuk át a predikciót a két esetben\n",
    "\n",
    "   1. P < N\n",
    "   \n",
    "      $ \\hat{y}_{test,under} = x_{test} \\cdot (X^TX)^{-1}X^TY $\n",
    "\n",
    "      $ = x_{test} \\cdot (X^TX)^{-1}X^T(X\\Theta^* + E) $\n",
    "\n",
    "      $ = x_{test} \\cdot (X^TX)^{-1}X^TX\\Theta^* + x_{test} \\cdot (X^TX)^{-1}X^TE $\n",
    "\n",
    "      $ = x_{test} \\cdot \\Theta^* + x_{test} \\cdot (X^TX)^{-1}X^TE $\n",
    "\n",
    "      $ x_{test} \\cdot \\Theta^* \\overset{def}{=} y^*_{test} $\n",
    "\n",
    "      $ \\hat{y}_{test,under} - y^*_{test} = x_{test} \\cdot (X^TX)^{-1}X^TE $\n",
    "\n",
    "      $ (X^TX)^{-1}X^T = X^+ = V \\Sigma^+ U^T$\n",
    "\n",
    "      $ \\hat{y}_{test,under} - y^*_{test} = x_{test} \\cdot V \\Sigma^+ U^TE = \\sum_{r=1}^{R} { 1 \\over \\sigma_r } (x_{test} \\cdot v_r)(u_r \\cdot E) $\n",
    "\n",
    "   2. P > N, a számítás hasonló (exercise for the reader)\n",
    "\n",
    "      $ \\hat{y}_{test,over} - y^*_{test} = \\sum_{r=1}^{R} { 1 \\over \\sigma_r } (x_{test} \\cdot v_r)(u_r \\cdot E) + x_{test} \\cdot (X^T(XX^T)^{-1}X-I_d)\\Theta^* $\n",
    "\n",
    "A két egyenlet arra mutat, hogy a teszt hibát 3 érték fog meghatározni:\n",
    "\n",
    "   1. Mennyire változnak a tanulási adatok minden irányban: $  1 \\over \\sigma_r $\n",
    "   2. Mennyire és milyen irányokban változnak a teszt adatok relatív a tanulási adatokhoz képest: $ x_{test} \\cdot v_r $\n",
    "   3. Mennyire képes az ideális modell megtanulni a tanulási adatokat: $ u_r \\cdot E $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308e1559",
   "metadata": {},
   "source": [
    "### Miért a legnagyobb a hiba az interpolációs küszöbön?\n",
    "\n",
    "![alt](geometric.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50db42",
   "metadata": {},
   "source": [
    "### Mikor nem történik Double Descent?\n",
    "\n",
    " - Nincsenek nullához közeli szinguláris értékek $(\\sigma_r)$\n",
    " - A teszt adatok nem változnak más irányokban mint a tanulási adatok\n",
    " - Az ideális model tökéletes illeszkedik a tanulási adathalmazra (E=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
